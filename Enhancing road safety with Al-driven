import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Step 1: Load the dataset
# Replace 'accidents.csv' with your real dataset
df = pd.read_csv('accidents.csv')

# Step 2: Basic overview
print(df.head())
print(df.info())

# Step 3: Data cleaning (dropping missing values for simplicity)
df = df.dropna()

# Step 4: Feature engineering
# Convert 'Time' to hour if present
if 'Time' in df.columns:
    df['Hour'] = pd.to_datetime(df['Time']).dt.hour

# Step 5: Data visualization
plt.figure(figsize=(10, 5))
sns.countplot(data=df, x='Severity')
plt.title('Accident Severity Distribution')
plt.show()

# Optional: Correlation heatmap
plt.figure(figsize=(10, 5))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Feature Correlation')
plt.show()

# Step 6: Predicting Accident Severity (as example target)
# Select features and target (adjust based on your dataset)
features = ['Hour', 'Weather_Condition', 'Visibility(mi)', 'Temperature(F)']
target = 'Severity'

# Convert categorical to numeric
df = pd.get_dummies(df, columns=['Weather_Condition'], drop_first=True)

X = df[features]
y = df[target]

# Step 7: Split and train model
X_train, X_test, y_train, y_test = train_test_split(X, y

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from xgboost import XGBClassifier
import seaborn as sns
import matplotlib.pyplot as plt
import folium
from folium.plugins import HeatMap

df = pd.read_csv('traffic_accidents.csv')

print("Initial Data Overview:")
print(df.head())
print(df.info())

df = df.dropna()

if 'Time' in df.columns:
    df['Hour'] = pd.to_datetime(df['Time'], errors='coerce').dt.hour
    df.dropna(subset=['Hour'], inplace=True)
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

target_column = 'Severity'  # Adjust if your target is named differently

if target_column not in df.columns:
    raise ValueError(f"Target column '{target_column}' not found in dataset!")

X = df.drop(columns=[target_column])
y = df[target_column]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("=== Classification Report ===")
print(classification_report(y_test, y_pred))

print("=== Confusion Matrix ===")
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

importance = model.feature_importances_
features = X.columns
sns.barplot(x=importance, y=features)
plt.title("Feature Importance")
plt.show()

if 'Latitude' in df.columns and 'Longitude' in df.columns:
    print("Generating Accident Hotspot Map...")
    accident_map = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()], zoom_start=10)
    heat_data = [[row['Latitude'], row['Longitude']] for index, row in df.iterrows()]
    HeatMap(heat_data).add_to(accident_map)
    accident_map.save("accident_hotspots.html")
    print("Map saved as 'accident_hotspots.html'")
else:
    print("No geolocation data (Latitude/Longitude) found for heatmap generation.")
